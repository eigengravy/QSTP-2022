{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FashionMNISTFeedForwardNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DNOZJJSSs90X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import torch.nn.functional as F \n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VSfMhx7uBnV",
        "outputId": "0c3b9df4-6e82-4a82-9bda-195a663d3e4f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0a5ee7a2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYIZWbipuITb",
        "outputId": "c68991d2-e261-4718-95f2-863569f6497b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/Colab Notebooks/fashion-mnist.csv\"\n",
        "fashion_mnist_csv = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "7KKSG2zKu0xT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTDataset(Dataset):\n",
        "    def __init__(self, data, transform = None):\n",
        "        self.fashion_MNIST = list(data.values)\n",
        "        self.transform = transform\n",
        "        \n",
        "        label = []\n",
        "        image = []\n",
        "        \n",
        "        for i in self.fashion_MNIST:\n",
        "            label.append(i[0])\n",
        "            image.append(i[1:])\n",
        "        self.labels = np.asarray(label)\n",
        "        self.images = np.asarray(image)/255\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        image = self.images[index]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "metadata": {
        "id": "hbKY8IFMtOf2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 0.8 \n",
        "TEST_SIZE = 1 - TRAIN_SIZE\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = FashionMNISTDataset(fashion_mnist_csv)\n",
        "\n",
        "TRAIN_DATASET_SIZE = int(len(dataset)*TRAIN_SIZE) \n",
        "TEST_DATASET_SIZE = len(dataset) - TRAIN_DATASET_SIZE\n",
        "train_set, test_set = random_split(dataset = dataset, lengths= [TRAIN_DATASET_SIZE,TEST_DATASET_SIZE])\n",
        "\n",
        "train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_set, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "UADW8t_Atl6e"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTNN(nn.Module):\n",
        "    def __init__(self, input_dim, h1,h2, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, h1)\n",
        "        self.fc2 = nn.Linear(h1, h2)\n",
        "        self.fc3 = nn.Linear(h2, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):                           \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = F.softmax(self.fc3(x),dim=1)                  \n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "fpz0LPeVtclW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(a, y):                            \n",
        "    a = torch.max(a, dim=1)[1]\n",
        "    assert not torch.any(torch.isnan(a))\n",
        "    return torch.mean((a == y).float()).item()"
      ],
      "metadata": {
        "id": "OSWDS9luw85Y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FashionMNISTNN(784,128,32,10)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=float(0.01))"
      ],
      "metadata": {
        "id": "K-A8nXfbtgNI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "loss_history = []\n",
        "accuracy_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    for image, label in train_loader:\n",
        "        image = image.view(-1, 784)\n",
        "        label = label.view(-1)\n",
        "        z = model(image.float())\n",
        "        loss = F.cross_entropy(z, label)\n",
        "        accuracy = calc_accuracy(z, label)\n",
        "\n",
        "        optimizer.zero_grad()      \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() / TRAIN_DATASET_SIZE * BATCH_SIZE\n",
        "        epoch_accuracy += accuracy / TRAIN_DATASET_SIZE * BATCH_SIZE\n",
        "    \n",
        "    loss_history.append(epoch_loss)\n",
        "    accuracy_history.append(epoch_accuracy)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch, round(epoch_loss, 3), round(100*epoch_accuracy, 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SFzZViwtgsN",
        "outputId": "e15a0cfb-b4a0-494d-f351-fcf9e98d542f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.875, Accuracy: 58.45\n",
            "Epoch: 5, Loss: 1.7, Accuracy: 76.038\n",
            "Epoch: 10, Loss: 1.685, Accuracy: 77.637\n",
            "Epoch: 15, Loss: 1.709, Accuracy: 75.225\n",
            "Epoch: 20, Loss: 1.691, Accuracy: 77.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "avg_loss = 0\n",
        "avg_accuracy = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    for image, label in test_loader:\n",
        "        z = model(image.float())\n",
        "        loss = F.cross_entropy(z, label)\n",
        "        accuracy = calc_accuracy(z, label)\n",
        "\n",
        "        epoch_loss += loss.item() / TEST_DATASET_SIZE * BATCH_SIZE\n",
        "        epoch_accuracy += accuracy / TEST_DATASET_SIZE * BATCH_SIZE\n",
        "\n",
        "    avg_loss += epoch_loss / num_epochs\n",
        "    avg_accuracy += epoch_accuracy / num_epochs\n",
        "\n",
        "print(\"Number of Epochs tested: {}, Average loss: {}, Average Accuracy: {}\".format(num_epochs, round(avg_loss, 3), round(100*avg_accuracy, 3)))"
      ],
      "metadata": {
        "id": "bnQhcoyS0OdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf588edd-dff9-4de5-e766-b68a3fafc469"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epochs tested: 5, Average loss: 1.764, Average Accuracy: 75.61\n"
          ]
        }
      ]
    }
  ]
}