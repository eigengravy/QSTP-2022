# -*- coding: utf-8 -*-
"""FashionMNISTFeedForwardNeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tGc7uNReR2d-w_aZ9FELdyVDqVau2oWU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
from torch import optim, nn
import torch.nn.functional as F 

from torchvision import transforms

from torch.utils.data import TensorDataset, DataLoader

torch.manual_seed(0)

from google.colab import drive
drive.mount('/content/drive')

path = "drive/MyDrive/Colab Notebooks/fashion-mnist.csv"
df = pd.read_csv(path)

X = torch.div(torch.Tensor(np.array(df.iloc[:, 1:])),255).type(torch.FloatTensor)
y = torch.Tensor(np.array(df.iloc[:, 0:1].values)).reshape(-1).type(torch.LongTensor)

batch_size = 64

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test,y_test)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

class FashionMNISTNN(nn.Module):
    def __init__(self, input_dim, h1,h2, output_dim):
        super().__init__()

        self.fc1 = nn.Linear(input_dim, h1)
        self.fc2 = nn.Linear(h1, h2)
        self.fc3 = nn.Linear(h2, output_dim)

        self.dropout = nn.Dropout(0.1)

    def forward(self, x):                           
        x = F.relu(self.fc1(x))
        x = self.dropout(F.relu(self.fc2(x)))
        x = F.softmax(self.fc3(x),dim=1)                  

        return x

def calc_accuracy(a, y):                            
    a = torch.max(a, dim=1)[1]
    assert not torch.any(torch.isnan(a))
    return torch.mean((a == y).float()).item()

model = FashionMNISTNN(784,128,32,10)

optimizer = optim.Adam(model.parameters(), lr=float(0.01))

num_epochs = 25
loss_history = []
accuracy_history = []

for epoch in range(num_epochs):
    epoch_loss = 0
    epoch_accuracy = 0
    for image, label in train_loader:
        z = model(image)
        loss = F.cross_entropy(z, label)
        accuracy = calc_accuracy(z, label)

        optimizer.zero_grad()      
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() / len(train_dataset) * batch_size
        epoch_accuracy += accuracy / len(train_dataset) * batch_size
    
    loss_history.append(epoch_loss)
    accuracy_history.append(epoch_accuracy)

    if epoch % 5 == 0:
        print("Epoch: {}, Loss: {}, Accuracy: {}".format(epoch, round(epoch_loss, 3), round(100*epoch_accuracy, 3)))

num_epochs = 5
avg_loss = 0
avg_accuracy = 0

for epoch in range(num_epochs):
    epoch_loss = 0
    epoch_accuracy = 0
    for image, label in test_loader:
        z = model(image.float())
        loss = F.cross_entropy(z, label)
        accuracy = calc_accuracy(z, label)

        epoch_loss += loss.item() / len(test_dataset) * batch_size
        epoch_accuracy += accuracy / len(test_dataset) * batch_size

    avg_loss += epoch_loss / num_epochs
    avg_accuracy += epoch_accuracy / num_epochs

print("Number of Epochs tested: {}, Average loss: {}, Average Accuracy: {}".format(num_epochs, round(avg_loss, 3), round(100*avg_accuracy, 3)))